{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6cfcc3a-76e3-46fd-8e1d-0d9f87f0617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b509e76-32c8-4cdb-8c81-4c7f5b29a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://baraasalout.github.io/test.html\"\n",
    "request = requests.get(url)\n",
    "soup = BeautifulSoup(request.content, 'html.parser') # parsing all the html of the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01f1c92e-19e6-42b5-9e49-2b3500232ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the wanted text and save it in a CSV file\n",
    "def extractText():\n",
    "    data = []\n",
    "    fields = ['Type', 'Content']\n",
    "\n",
    "    # extracting the content of every type of text as a list\n",
    "    h1 = soup.find_all(\"h1\")\n",
    "    h2 = soup.find_all(\"h2\")\n",
    "    p = soup.find_all(\"p\")\n",
    "    li = soup.find_all(\"li\")\n",
    "\n",
    "    # looping through each type to print it and add it to the data list as a dictionary containing the type of the text and it's content\n",
    "    print(\"All h1: \")\n",
    "    for i in h1:\n",
    "        print(i.text)\n",
    "        data.append({\"Type\": \"Heading 1\", \"Content\": i.text})\n",
    "\n",
    "    print(\"\\n All h2: \")\n",
    "    for i in h2:\n",
    "        print(i.text)\n",
    "        data.append({\"Type\": \"Heading 2\", \"Content\": i.text})\n",
    "\n",
    "    print(\"\\n All p: \")\n",
    "    for i in p:\n",
    "        print(i.text)\n",
    "        data.append({\"Type\": \"Paragragh\", \"Content\": i.text})\n",
    "\n",
    "    print(\"\\n All li: \")\n",
    "    for i in li:\n",
    "        print(i.text)\n",
    "        data.append({\"Type\": \"List item\", \"Content\": i.text})\n",
    "\n",
    "    # Now we create the CSV file and fill it with the content in the data list\n",
    "    with open('Extract_Text_data.csv', 'w', newline = '', encoding = 'utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fields)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f1d44a3-9717-4138-8400-995eac3a3b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All h1: \n",
      "Web Scraping Practice\n",
      "\n",
      " All h2: \n",
      "Available Products\n",
      "Product Table\n",
      "Watch This Video\n",
      "Contact Us\n",
      "Product Information\n",
      "Featured Products\n",
      "\n",
      " All p: \n",
      "Welcome to the web scraping task! Use your skills to extract the required data from this page.\n",
      "Sharp Objects\n",
      "£47.82\n",
      "✔ In stock\n",
      "In a Dark, Dark Wood\n",
      "£19.63\n",
      "✔ In stock\n",
      "The Past Never Ends\n",
      "£56.50\n",
      "✔ In stock\n",
      "A Murder in Time\n",
      "£16.64\n",
      " Out stock\n",
      "Wireless Headphones\n",
      "$49.99\n",
      "Available colors: Black, White, Blue\n",
      "Smart Speaker\n",
      "$89.99\n",
      "Available colors: Grey, Black\n",
      "Smart Watch\n",
      "$149.99\n",
      "Available colors: Black, Silver, Gold\n",
      "© 2024 Web Scraping Practice. All Rights Reserved.\n",
      "\n",
      " All li: \n",
      "Laptop\n",
      "Smartphone\n",
      "Tablet\n",
      "Smartwatch\n"
     ]
    }
   ],
   "source": [
    "extractText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a76df31e-2ace-4751-a305-c2424c386db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract the content out of the table and save it in CSV file\n",
    "def extractTable():\n",
    "    # defin the data list and the heading of the table \n",
    "    data = []\n",
    "    fields = [\"Product\", 'Price', \"In Stock\"]\n",
    "\n",
    "    #Extract the table and it's rows as a list\n",
    "    table = soup.find('table')\n",
    "    rows = table.find_all('tr')\n",
    "    \n",
    "\n",
    "    #now we loop through each row starting form the second one(Becoause the first one is the header), then the data is added as a dictionary to the data list the\n",
    "    for i in range(1, len(rows)):\n",
    "        cells = rows[i].find_all('td')\n",
    "        # print(cells[0])\n",
    "        data.append({\"Product\": cells[0].text, \"Price\": cells[1].text, \"In Stock\": cells[2].text})\n",
    "\n",
    "    # then the CSV file is created and filled with the data list\n",
    "    with open('Extract_Table_Data.csv', 'w', newline = '', encoding = 'utf-8') as file: \n",
    "        writer = csv.DictWriter(file, fieldnames = fields)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "    for i in data:\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beb3889b-ba79-48b7-985a-da0724fd2d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Product': 'Laptop', 'Price': '$1000', 'In Stock': 'Yes'}, {'Product': 'Smartphone', 'Price': '$800', 'In Stock': 'No'}, {'Product': 'Tablet', 'Price': '$500', 'In Stock': 'Yes'}]\n",
      "[{'Product': 'Laptop', 'Price': '$1000', 'In Stock': 'Yes'}, {'Product': 'Smartphone', 'Price': '$800', 'In Stock': 'No'}, {'Product': 'Tablet', 'Price': '$500', 'In Stock': 'Yes'}]\n",
      "[{'Product': 'Laptop', 'Price': '$1000', 'In Stock': 'Yes'}, {'Product': 'Smartphone', 'Price': '$800', 'In Stock': 'No'}, {'Product': 'Tablet', 'Price': '$500', 'In Stock': 'Yes'}]\n"
     ]
    }
   ],
   "source": [
    "extractTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1ba4e15-444b-4fc8-b52a-2f4cf7e9c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a Function to extract the data from the book cards section in the website as save it in a JSON file\n",
    "def extractBookCards():\n",
    "    data = []\n",
    "    #Extract the cards data from the website as a list using the class of it's container\n",
    "    cardsContainer = soup.find(\"div\", class_='book-products')\n",
    "    cards = cardsContainer.find_all('div')\n",
    "\n",
    "    # loop through the cards and get all the text from it and the text of it's button then add each one in a dictionary in the data list\n",
    "    for card in cards:\n",
    "        p = card.find_all('p')\n",
    "        button = card.find('button')\n",
    "\n",
    "        data.append({'Book title':p[0].text, 'Price': p[1].text, 'Stock availablity': p[2].text, \"Button text\": button.text })\n",
    "\n",
    "    #Create the JSON file and add the data from the data list\n",
    "    with open(\"Product_Information.json\", 'w') as file:\n",
    "        json.dump(data, file)\n",
    "\n",
    "    for i in data:\n",
    "        print(i)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "163cede2-28cc-47b7-bdc4-798ee67ae296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Book title': 'Sharp Objects', 'Price': '£47.82', 'Stock availablity': '✔ In stock', 'Button text': 'Add to basket'}\n",
      "{'Book title': 'In a Dark, Dark Wood', 'Price': '£19.63', 'Stock availablity': '✔ In stock', 'Button text': 'Add to basket'}\n",
      "{'Book title': 'The Past Never Ends', 'Price': '£56.50', 'Stock availablity': '✔ In stock', 'Button text': 'Add to basket'}\n",
      "{'Book title': 'A Murder in Time', 'Price': '£16.64', 'Stock availablity': ' Out stock', 'Button text': 'Add to basket'}\n"
     ]
    }
   ],
   "source": [
    "extractBookCards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4801e5fa-0ce4-442b-b044-8f5e9de38ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract the data from the form in the page and save it in a JSON \n",
    "def extractFormDetails():\n",
    "    data = []   \n",
    "    #extract the form and get all types of tags inside it even if it was label or input\n",
    "    form = soup.find('form')\n",
    "    inputs = form.find_all(True)\n",
    "    \n",
    "    # now we filter the data in the variable input by removing the labels from it\n",
    "    cleanInputs = [i for i in inputs if i.name != 'label']\n",
    "    # then we loop throu the rest of the clean input  and add it's data to the data list as a dictionary\n",
    "    for i in cleanInputs:\n",
    "        inputType = i.get('type') or i.name\n",
    "        data.append({\"Field name\": i.get('name'), \"Input type\": inputType, \"Default value\": i.get(\"value\"), \"Place holder\": i.get(\"placeholder\")})\n",
    "        \n",
    "    # Create the JSON file and add the data from the data list \n",
    "    with open('Extract_Form_details.json', 'w') as file:\n",
    "        json.dump(data, file)\n",
    "    \n",
    "    for i in data:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c68f6bfa-85b2-462b-b4d3-830cff55ad97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Field name': 'username', 'Input type': 'text', 'Default value': None, 'Place holder': 'Enter your username'}\n",
      "{'Field name': 'password', 'Input type': 'password', 'Default value': None, 'Place holder': 'Enter your password'}\n",
      "{'Field name': 'options', 'Input type': 'select', 'Default value': None, 'Place holder': None}\n",
      "{'Field name': None, 'Input type': 'option', 'Default value': 'option1', 'Place holder': None}\n",
      "{'Field name': None, 'Input type': 'option', 'Default value': 'option2', 'Place holder': None}\n",
      "{'Field name': None, 'Input type': 'option', 'Default value': 'option3', 'Place holder': None}\n",
      "{'Field name': 'terms', 'Input type': 'checkbox', 'Default value': None, 'Place holder': None}\n",
      "{'Field name': None, 'Input type': 'submit', 'Default value': 'Submit', 'Place holder': None}\n"
     ]
    }
   ],
   "source": [
    "extractFormDetails()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "380d52f1-a8fd-4e41-8644-47cdb8a9df57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function which extracts media from the web page either it was a hyperlink or an image or a video then add it's link in a JSON file\n",
    "def extractMedia():\n",
    "    data = []\n",
    "\n",
    "    # extract all the links and videos (and images as bonus from me) from the page using it's tags then loop on it \n",
    "    # and add it to the data list in form of a dictionary which cantains the value of the type of the tage and the link from the src attribute\n",
    "    links = soup.find_all(\"a\")    #There is no <a> tags in the website But just in case\n",
    "    for link in links:\n",
    "        data.append({'Type': \"Link\", 'Link': link.get('href')})\n",
    "        \n",
    "    videos = soup.find_all('iframe')     #I meant to use find_all just in case more videos are added later although there is anly one video in the page \n",
    "    for video in videos:\n",
    "        data.append({'Type': \"Video\", 'Link': video.get('src')})\n",
    "\n",
    "    images = soup.find_all('img')\n",
    "    for image in images:\n",
    "        data.append({'Type': \"Image\", 'Link': image.get('src')})\n",
    "\n",
    "    # save the data in the JSON file\n",
    "    with open(\"Extract_Links.json\", 'w') as file:\n",
    "        json.dump(data, file)\n",
    "\n",
    "    for i in data:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75d73ef9-7d6b-4241-b1a3-d018d25be2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Type': 'Video', 'Link': 'https://www.youtube.com/watch?v=ujf9RNuBdCU'}\n",
      "{'Type': 'Image', 'Link': 'http://books.toscrape.com/media/cache/26/0c/260c6ae16bce31c8f8c95daddd9f4a1c.jpg'}\n",
      "{'Type': 'Image', 'Link': 'http://books.toscrape.com/media/cache/92/27/92274a95b7c251fea59a2b8a78275ab4.jpg'}\n",
      "{'Type': 'Image', 'Link': 'http://books.toscrape.com/media/cache/c0/59/c05972805aa7201171b8fc71a5b00292.jpg'}\n",
      "{'Type': 'Image', 'Link': 'http://books.toscrape.com/media/cache/97/27/97275841c81e66d53bf9313cba06f23e.jpg'}\n",
      "{'Type': 'Image', 'Link': 'https://via.placeholder.com/250x150?text=Product+1'}\n",
      "{'Type': 'Image', 'Link': 'https://via.placeholder.com/250x150?text=Product+2'}\n",
      "{'Type': 'Image', 'Link': 'https://via.placeholder.com/250x150?text=Product+3'}\n"
     ]
    }
   ],
   "source": [
    "extractMedia()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c95f90d-8612-4f18-a051-316734215e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a scripting challenge function\n",
    "# it extracts the data from the cards in featured products section in the webpage including the name and the id of the product and it's hidden price from the attributes \n",
    "def scriptingChallenge():\n",
    "    data = []\n",
    "    fields = [\"Product Name\", \"Hidden Price\", 'Available Colors', 'Product ID']\n",
    "\n",
    "    # extract the cards from the featured products section as a list using the class names of the cards and it's container\n",
    "    products = soup.find('div', class_='products').find_all('div', class_ = 'product-card')\n",
    "\n",
    "    # loop through the products and get the wanted data either from it's child tags with class names or from it's attributes \n",
    "    # Add the data to the data list as dictionaries containing the wanted data\n",
    "    for product in products:\n",
    "        data.append({'Product Name': product.find('p', class_='name').text, \n",
    "                    \"Hidden Price\": product.find('p', class_='price').text,\n",
    "                    'Available Colors': product.find('p', class_='colors').text,\n",
    "                    'Product ID': product.get('data-id')})\n",
    "    # create a CSV file to save the data from the data list\n",
    "    with open('Featured_products_data.csv', 'w', newline = '') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames = fields)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "    \n",
    "    for i in data:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbeb3433-aa8a-4c66-922d-87cc43110196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Product Name': 'Wireless Headphones', 'Hidden Price': '$49.99', 'Available Colors': 'Available colors: Black, White, Blue', 'Product ID': '101'}\n",
      "{'Product Name': 'Smart Speaker', 'Hidden Price': '$89.99', 'Available Colors': 'Available colors: Grey, Black', 'Product ID': '102'}\n",
      "{'Product Name': 'Smart Watch', 'Hidden Price': '$149.99', 'Available Colors': 'Available colors: Black, Silver, Gold', 'Product ID': '103'}\n"
     ]
    }
   ],
   "source": [
    "scriptingChallenge()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
